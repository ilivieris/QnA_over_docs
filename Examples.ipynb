{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
    "\n",
    "from langchain.document_loaders import TextLoader  #for textfiles\n",
    "from langchain.text_splitter import CharacterTextSplitter #text splitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings #for using HugginFace models\n",
    "# Vectorstore: https://python.langchain.com/en/latest/modules/indexes/vectorstores.html\n",
    "# from langchain.vectorstores import FAISS  #facebook vectorizationfrom langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "import textwrap\n",
    "\n",
    "def wrap_text_preserve_newlines(text, width=110):\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Loader\n",
    "loader = DirectoryLoader('./Data', glob=\"*.txt\")\n",
    "documents = loader.load()\n",
    "   \n",
    "# print(wrap_text_preserve_newlines(str(documents[0]))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Number of docs:  0\n"
     ]
    }
   ],
   "source": [
    "# Text Splitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, separator='\\n', chunk_overlap=10)\n",
    "\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print('[INFO] Number of docs: ', len(docs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39m# Create the vectorized db\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m# Vectorstore: https://python.langchain.com/en/latest/modules/indexes/vectorstores.html\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvectorstores\u001b[39;00m \u001b[39mimport\u001b[39;00m FAISS\n\u001b[1;32m----> 7\u001b[0m db \u001b[39m=\u001b[39m FAISS\u001b[39m.\u001b[39;49mfrom_documents(docs, embeddings)\n",
      "File \u001b[1;32mc:\\Users\\livieris\\.conda\\envs\\LangChain\\lib\\site-packages\\langchain\\vectorstores\\base.py:296\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m texts \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mpage_content \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[0;32m    295\u001b[0m metadatas \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mmetadata \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[1;32m--> 296\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mfrom_texts(texts, embedding, metadatas\u001b[39m=\u001b[39mmetadatas, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\livieris\\.conda\\envs\\LangChain\\lib\\site-packages\\langchain\\vectorstores\\faiss.py:421\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, **kwargs)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \n\u001b[0;32m    405\u001b[0m \u001b[39mThis is a user friendly interface that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[39m        faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    420\u001b[0m embeddings \u001b[39m=\u001b[39m embedding\u001b[39m.\u001b[39membed_documents(texts)\n\u001b[1;32m--> 421\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m__from(\n\u001b[0;32m    422\u001b[0m     texts,\n\u001b[0;32m    423\u001b[0m     embeddings,\n\u001b[0;32m    424\u001b[0m     embedding,\n\u001b[0;32m    425\u001b[0m     metadatas,\n\u001b[0;32m    426\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    427\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\livieris\\.conda\\envs\\LangChain\\lib\\site-packages\\langchain\\vectorstores\\faiss.py:373\u001b[0m, in \u001b[0;36mFAISS.__from\u001b[1;34m(cls, texts, embeddings, embedding, metadatas, normalize_L2, **kwargs)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    363\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__from\u001b[39m(\n\u001b[0;32m    364\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    371\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m FAISS:\n\u001b[0;32m    372\u001b[0m     faiss \u001b[39m=\u001b[39m dependable_faiss_import()\n\u001b[1;32m--> 373\u001b[0m     index \u001b[39m=\u001b[39m faiss\u001b[39m.\u001b[39mIndexFlatL2(\u001b[39mlen\u001b[39m(embeddings[\u001b[39m0\u001b[39;49m]))\n\u001b[0;32m    374\u001b[0m     vector \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(embeddings, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m    375\u001b[0m     \u001b[39mif\u001b[39;00m normalize_L2:\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\", \n",
    "                                   model_kwargs={'device': 'cpu'})\n",
    "\n",
    "# Create the vectorized db\n",
    "# Vectorstore: https://python.langchain.com/en/latest/modules/indexes/vectorstores.html\n",
    "from langchain.vectorstores import FAISS\n",
    "db = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Which countries participated in Napoleonic Wars?\"\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "print('[INFO] Number of docs: ', len(docs))\n",
    "print(wrap_text_preserve_newlines(str(docs[0].page_content)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=HuggingFaceHub(repo_id=\"google/flan-t5-small\", model_kwargs={\"temperature\":0, \"max_length\":4096}) \n",
    "# llm=HuggingFaceHub(repo_id=\"declare-lab/flan-alpaca-large\", model_kwargs={\"temperature\":0, \"max_length\":4096}) \n",
    "\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Which were the main reasons, which lead to the French Revolution?\"\n",
    "docs = db.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)\n",
    "\n",
    "# Answer: 'widespread social distress'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Napoleon participated in the French Revolution?\"\n",
    "docs = db.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)\n",
    "\n",
    "# Answer: 'He supported the French Revolution in 1789 while serving in the French army'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who was the losser in Napoleonic Wars?\"\n",
    "docs = db.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)\n",
    "\n",
    "# Answer: \"The Napoleonic Wars ended in French defeat with France occupied under the Treaty of Paris (1815), \n",
    "# Napoleon's rule over and the Bourbon Monarchy restored\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"When did Napoleon died?\"\n",
    "docs = db.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)\n",
    "\n",
    "# Answer: '5 May 1821'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"When was Napoleon born?\"\n",
    "docs = db.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)\n",
    "\n",
    "# Answer: '15 August 1769'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who was Napoleon's greatest enemy\"\n",
    "docs = db.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)\n",
    "\n",
    "# Answer: 'Adolf Hitler'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What were the outcomes of the French Revolution\"\n",
    "docs = db.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)\n",
    "\n",
    "# Answer: 'The revolution was the most significant challenge to political absolutism up to that point in history and spread democratic ideals throughout Europe and ultimately the world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Question Answering with local persistence\n",
    "# An example of using Chroma DB and LangChain to do question answering over documents, with a locally persisted database. You can store embeddings and documents, then use them again later.\n",
    "\n",
    "# https://github.com/hwchase17/chroma-langchain/blob/master/persistent-qa.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
